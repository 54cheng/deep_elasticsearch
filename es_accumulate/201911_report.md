# 1、count api 和 直接设置size = 0 ， 来统计条数的区别。
官方提供了count api 来获取匹配的结果， 如下：
GET /twitter/tweet/_count
{
    "query" : {
        "term" : { "user" : "kimchy" }
    }
}
那么，按照下面这种方法，和count 方法有何区别：
 
GET /twitter/tweet/_search
{
    "size": 0,
    "query" : {
        "term" : { "user" : "kimchy" }
    }
}
结果都可以得到相应的总的匹配的条数。 

回复：
上面返回值中有一个"relation" : "gte"表示返回的文档数超过10000个。不是精确的，如果你想得到精确的返回值，你可以这么做：
 
GET _search

{

"track_total_hits": true

}
这样我们就可以返回所有的文档

# 2、ES查询功能上线，导致线程dump，有什么好的方法优化？
回复：
ES是需要非常强大的硬件环境支持，如果硬件不够，基本就是demo的演示。
 
ES是高消耗内存，高消耗cpu，高消耗磁盘空间的一款开源全文检索引擎，它不是神，它的最适合的场景是全文检索。
 
cpu核心数越多，对查询来说越好，如果有可能，最好采用40core cpu的 物理机
 
内存越大越好，最好是64GB物理内存，如果有可能，一台40core 64gb内存的服务器，最大存储数据量在5TB左右。
 
硬盘空间要合理，最好是储存数据的3.4倍(参考阿里的算法),假如存储5TB的数据，磁盘空间最好是15TB左右。最大不要超过20TB。
 
按照经验，当128GB内存，分配给es64gb堆内存时，存储的数据上限是15TB（最好不要超过10TB），否则出现各种问题，尤其是内存full gc。

# 3、如何修改search.max_buckets
PUT _cluster/settings
{
  "persistent": {
    "search.max_buckets": 20000
  }
# 4、大佬们，有一个疑惑，既然在写入文档时，有机会和时间去写translog到磁盘，为什么还要再费劲去写内存呢？

个人认为这是对分布式应用的一个很好的实践吧，保证数据不丢失的同时尽量提高性能...
写buffer可以直接响应，提高响应性能。。但是又不能丢失内存数据，所以再写一份持久化log。。。最后，最终一致性（刷盘或重启读log）落盘segment.

性能与可靠性之间的博弈

# 5、composite 多字段聚合按照count值降序返回
1.es6.8版本
2.目前使用composite 方式实现多字段聚合，遇到的问题就是想按照count值降序返回结果， 但是官方文档没有说明该方式， 只列举了按照key的降序和升序返回的方式， 请问有遇到类似情况的吗， 指教一下
官方7.5 推出了稀有聚合 rare terms aggregation 就是解决您的问题的。建议看一下。

https://elasticsearch.cn/question/8715

# 6、es里面是应该建多个索引，还是建一个索引进行查询更新性能更好
比如说：task有：user、title、content、thirdpart_name,thirdpart_url;
里面是user和thirtpart 在Mysql都是单独建了一张表，然后通过user_id，和thirdpart_id进行关联。

es7.3需要也拆成多个索引进行管理吗？刚使用es，不知道怎么定方向

回复：ES 不适合多表 join 查询，因此推荐先 join 后存储到 ES 里面，存储单张表查询性能最佳。
 
当子表发生数据变化的时候可以使用 UpdateByQuery 去更新

# 7、ES索引设计问题

因为网上有建议新手学习开源开发，从比较高的版本开始 ，刚开始学习使用 es  7.3.2，在搭建一套 EL+自己的查询 系统，用来采集，查看，公司客户的云上应用日志
现有一个问题，本想按 一个应用一个index，type 则对应 应用的不同文件 err.log info.log 等，如果客户应用数量达到了上万个，甚至十万个，这个。。。。我看有人说 3000个index，就已经很慢了;
index的数量到底会不影响查询速度？
 
 如果 10万个index   里面包含 100亿级别数据  和 多个index 一起包含 100亿数据，他们之间的查询性能应该不一样吧，查询可以指定 index，范围会小很多，理论上速度会更快不是？
index 的数量 和 里面包含的数据量怎么样来平衡呢？

回复：
一定要记住这句话， 生产环境，ES集群是有极限的。
 
一个集群是有最大节点限制，最大内存限制，最大磁盘空间限制的。一个集群最大索引数，最大分片数都是有限制的。
 
一个集群，最大节点最好不要超过20个，（分不同的场景）。一个节点的内存最好是64GB，或者128GB，硬盘最好是10TB或者20TB，
cpu 至少20核心，最好40核心。
 
你这种场景，不应该只用一个集群， 接入一种业务，一个集群比较好(按照你的描述，业务就是你接入的应用)。
 
这样的最有扩展的，最大隔离不用的业务，相互之间无影响，一个集群挂掉，不影响其他集群业务。
 
千万不要所有业务只用一个集群，所有索引放在一个集群。
 

# 8、多节点部署的ES做snapshot时，为什么要配置共享文件系

最近在做ES的灾备，发现大家的做法都是使用sshf指定一个节点的磁盘作为共享磁盘，让所有节点往里边写，为什么不能指定一个节点让它做呢，一定要用共享文件系统？

回复：肯定要共享文件系统的，master在更新完快照元数据后，会用ClusterState的方式通知索引主分片所在的节点进行数据拷贝，所有数据都会放一起。
如果不是共享的，你想想会出现什么问题？

# 9、es性能测试：抛出大量429错误，CPU利用率仅70%，如何让CPU利用率跑到90%以上？

测试拓扑：1个master节点（CPU2核，服务器内存8G），2个数据节点（CPU2核，服务器内存32G）。三个节点分别部署在三台服务器上。

测试表现：向数据节点1发送写入请求，写入速率约为600个请求每秒，大量请求返回429（reject，队列满）。但数据节点1、2的CPU利用率仅百分60%，内存60%。并未达到性能极限。

问题1：为何抛出大量429错误，CPU却没有跑满？
问题2：如何让CPU跑到90%以上？

回复：
1、大量请求返回429（reject，队列满）,这就说明 写入 已经达到极限了啊，都拒绝了。插入是不会消耗CPU的，除非有其他的连带操作。
比如插入的过程中，Elasticsearch 会做段文件合并，会消耗大量的cpu和load,当然也会消耗io。
一般对于es集群，都是单生产者，多消费者，或者几个生产者。而你测试的时候，好像是使用了大量的生产者去插入。
难道是想实现向mysql一样的，大量的增加，删除，修改，功能吗？

2、es 不符合这种场景。并发修改，会导致冲突的。
ES 并不适合并发修改， 有增删改需求还是考虑其他软件吧。

ES 是一个全文检索框架，是专门应付全文检索查询和简单少数据的聚合查询的。
如果是单个生产者，进行增删改，那是没有问题的。

并发增删改不行。

# 10、Elasticsearch 单独分离协调节点意义在哪
来社区请教一下大佬：
 我在一个节点上的配置如下：
node.master: false
node.data: false
node.ingest: false
 
我想用这个节点作为一个仅协调节点，让这个节点做集群的负载均衡。但是默认每一个节点都是一个协调节点，请求也会打到其他节点上。那把这个节点单独隔离的意义在哪里呢？

回复：
5.X版本之后，没有client节点，确切的说较：协调节点。
请仔细读一下协调节点的作用：

第一：诸如搜索请求或批量索引请求之类的请求可能涉及保存在不同数据节点上的数据。 例如，搜索请求在两个阶段中执行（query 和 fetch），这两个阶段由接收客户端请求的节点 - 协调节点协调。

第二：在请求阶段，协调节点将请求转发到保存数据的数据节点。 每个数据节点在本地执行请求并将其结果返回给协调节点。 在收集fetch阶段，协调节点将每个数据节点的结果汇集为单个全局结果集。

第三：每个节点都隐式地是一个协调节点。 这意味着将所有三个node.master，node.data和node.ingest设置为false的节点作为仅用作协调节点，无法禁用该节点。 结果，这样的节点需要具有足够的内存和CPU以便处理收集阶段。

# 11、【重要】关于ES7.X移除Transport Client 性能疑惑
ES7.*移除Transport Client.关于原因官方虽然有解释但是都没提到性能问题
这里有篇官方的博客 https://www.elastic.co/cn/blog/benchmarking-rest-client-transport-client
对比了Rest Client和Transport Client之间的性能问题.
但是测试基准是单线程客户端.
Transport Client是基于Netty实现的tcp客户端.非阻塞IO 多路复用 在高并发下表现很好
但是Rest Client是基于HTTP实现的.HTTP1.1不支持多路复用.在高并发下性能应该会下降的很明显

Medcl回复：性能是会有一些影响，不过现实情况，这些差异你根本不用担心的，使用 HTTP 代替 TCP 更多的是避免版本耦合的问题，以前服务器和客户端版本不一致，会出现两边对象无法反序列化的问题，如果用 TCP，升级服务端，客户端也得都升级，什么滚动升级基本上也别想了。 此为 HTTP 性能已经足够强悍了，并发吞吐同样可以达到很高，更多的时候是其它资源先不够，此外 Rest协议底层一样也是基于 Netty 的。

https://elasticsearch.cn/question/8796

# 12、【星球小伙伴貌似问过】elasticsearch单字导致大结果集召回，性能出现问题，有好的方案解决吗？
实际业务中，使用es6.x版本，其中文档数量在5亿左右，当用户输入单字，如：好，这种字的时候，容易召回上百万的结果集，导致function_score逻辑耗时超过2s。请问大家碰到这种情况是如何解决的？

回复：
（1）如果只是单字搜索不想耗时，那么：
目前兜底的打分模板不动，针对单字走另一个模板，该模板里的所有function_score匹配域保留重要字段即可，比如去掉描述、正文。因为单字场景用户提供的信息极少，用标题match+文档质量分足够出好文档。
（2）如果所有搜索词都想提速，那么：
修改原有打分模板，function_score保留原有样子，在filter里追加一个must去包一个/几个match或term，通过减少字段域或minimum_should_match往更严格的方向去卡匹配阈值，通过此来减少倒排求交召回的文档数量。阈值别太严格，不影响目标文档被召回即可。

# 13、ES 6.7.2中使用term query 查询 浮点类型数据出现问题
今天使用term query 查询一个浮点类型，例如12.3。发现无法匹配，然后看了一下mapping，发现type是long类型。
查询参数如下：
```
{
    "query": {
        "bool": {
            "must": [
                {
                    "term": {
                        "knInfoExtend.allowanceTotal": {
                            "value": 15.5
                        }
                    }
                }
            ]
        }
    }
}
```

text我知道可以，但是有没有解释一下为什么这样不行
回复：问得很好，因为倒排表必须存离散型数据，一旦支持数值型，哪怕域只是[0-1]，表就炸了。token都是0.001、0.0001、0.0000001...原理上不支持

# 14、【探讨】index.refresh_interval能设置到多小，有没有一个参考值

目前需要将数据写入，但需要马上搜索对其修改。服务器是16g内存，500g固态，8核的cpu。由于写入到能被搜索存在一定的间隔时间，所以现在并发操作数据经常报409版本冲突错误，所以想将间隔时间设置小一点。目前设置是500ms。但不知道再往下设置，会不会造成es其他问题。

回复：
refresh_interval控制刷新内存到lucene段的生成速度，在高并发大数据量写入的情况下，时间太小会导致太多的大量小段生成，对后台段的merge以及查询需要访问更多的段都有影响，造成集群不稳定，不建议再调低。
 
还有一个，ES不太适合写完马上就修改的场景，修改写入的数据实际上在lucene也是新生成了一条记录。对于这种实时可见的场景，不建议用ES, 可以试试mongo或者在应用层端自己合并在ES中查询的结果。

https://elasticsearch.cn/question/8901

# 15、关于es聚合相关优化慢——全量聚合
https://elasticsearch.cn/question/8876
回复1:

1核心：size":2147483647, 确认是否必须
2,你是多重嵌套，势必会慢
3,如何排查慢？推荐：profile:true 定位慢的根因
推荐：https://mp.weixin.qq.com/s/RTpBaFpNELQCO6VE0KMfsw

回复2：
这么说吧，所有的优化，都是因为需求不合理或者硬件资源不够导致的。
全量数据聚合，为什么要用ES呢？
而且你可以选择离线聚合一次保存到另一个，以后的查询可以走另一个索引。
3s已经非常快了。

因为我读了那块es聚合的源码。在terms聚合的时候，有一个bucket优先队列，瓶颈在于size的数目越多，对于聚合的瓶颈越大。想看看有没有从源码上面级别去优化。
如果全量数据聚合的话，有没有其他的中间件合适？

希望大家探讨一下！

# 16、logstash性能调优
问题：logstash读取kafka数据输出到elasticsearch，在kafka堆积了大量的数据，cpu使用率到达98%，
服务器环境是16c64g

调优：
1. pipeline.batch.delay: 5这个可以调长一点,比如50
2.部署多个logstash
3.试试携程的hangout
 

# 17、如何优雅地关闭elasticsearch集群？

es版本5.6
由于某些原因，需要关闭整个es集群并隔一天后重新开启，请教下建议的关闭操作是什么？
查了下_shutdown这个API已经在2.x版本之后就废弃了，网上建议的方法比较复杂https://blog.csdn.net/wangzhen3798/article/details/84069909，有没有比较推荐的操作方法？

回复：
参考官网的：
Full cluster restart upgrade
https://www.elastic.co/guide/en/elasticsearch/reference/current/restart-upgrade.html
将其中升级部分去掉就好了，

核心步骤：
1、关闭集群自动分配——确保不再写入；
2、同步写盘——确保数据不丢失
3、逐台关闭节点。

# 18、跨多个月存储索引数据的冷热数据分离如何实现？
有个问题请教一下，我用elasticsearch存订单数据，在热节点保存3个月前的数据（用一个索引），在冷节点保存3个月后的数据（用一个索引），数据会先持续写入热节点。目前找到的冷热数据分离方案，都是基于按时间序列建多个hot索引（比如1天1个索引），过了几天后在把整个索引放入冷节点。
而我的业务场景是只有一个hot索引，要把里面的数据按条件定时迁移到另外一个索引，因为业务要求能跨3个月或整年查数据，请问大家有什么好的建议吗？ 或者说我这个思路行不通啊？？ 谢谢。

我现在想到的初步方案是：通过JAVA API每天定时轮循从热节点查出来在写入冷节点。但感觉这个方法不好，如有好的建议请指导一下，谢谢。

wood大叔回复：
每天从热节点查出来写入冷节点这方案不靠谱，开销太高了。  你就根据单日订单量的大小，一天一个索引，或者一个月一个索引的方式来建就好了，最经济易维护的冷热数据分离方案。 因为搜索是可以跨多个索引的，所以这种建索引的方式可以满足业务上要求跨3隔越或者跨整年查询。  

索引按天创建的化，日期可以作为后缀加到索引名字里，形如 "orders-YYYYMMDD" 。 代码里根据用户传入的搜索时间范围，转化成需要搜索的索引列表。 ES的搜索API构建Client时支持传入一组名称作为搜索的目标索引。 打分、排序、分页和搜单个索引没什么区别。

