# 1、elasticsearch 重启第一次查询太慢

集群中有50亿数据，重启es集群，执行echo 3 > /proc/sys/vm/drop_cache之后单条件精确匹配时间有7s多，

io跑满从磁盘读大量数据，第二次执行则几百毫秒，如何提升第一次查询的速度呢?

https://elasticsearch.cn/question/9511

【回复】

warm_up_the_filesystem_cache

https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-search-speed.html#_warm_up_the_filesystem_cache

# 2、logstash应该如何同步到List形式的ES对象字段呢

请教各位老哥，小弟用logstash从PG数据库中同步数据到ES（logstash和ES版本都是6.5.2），需要同步的数据如图

https://elasticsearch.cn/question/9752

【medcl 回复】

这里主要是看如何合并多条记录到一个文档里面，首先要看这些记录是不是都是静态的，如果有更新则还要考虑更新的问题。有多种实现方式：
 
一种是在 Logstash 里面按主 key 排序，根据主 key 在内存里面合并成一个完整的对象，判断出现新的 key 的时候，发送到 Output 里面。

具体可以参照 Aggregate Filter的用法：https://www.elastic.co/guide/en/logstash/current/plugins-filters-aggregate.html
 
另外一种则是用 es 端来做 upsert 聚合，具体参照我的这篇文章：https://mp.weixin.qq.com/s/H1I3EeXEaOWpEBSR_SRBow

注意这种方式会对 Elasticsearch 造成额外的更新压力，适合数据量不大的场景。
 
如果数据量很大，还是建议自己写程序来合并封装，要更加灵活和可控，性能也会好一点。

# 3、es在搜索无输入关键词下使用boost权重

现有一个索引，a。字段有content-句子，times-句子使用频次，想通过用户关键词匹配匹配到content句子同时使用频次作为权重优化输出句子，

但是频次无法使用具体值，只能作为一个权重，所以想通过直接为频次设置boost权重来搜索句子，es并不支持在无键入情况下使用boost，这种问题该如何解决呢？
es使用版本 2.x

【回复】
参考：

```
GET /_search
{
    "query": {
        "function_score": {
            "field_value_factor": {
                "field": "likes",
                "factor": 1.2,
                "modifier": "sqrt",
                "missing": 1
            }
        }
    }
}
```

# 4、java.security.AccessControlException

https://elasticsearch.cn/question/9765

自己编写的插件中访问了localhost:0的socket，但是运行es后，就会出现，create a new socket fail!
java.security.AccessControlException: access denied ("java.net.SocketPermission" "localhost:0" "listen,resolve")

【medcl 回复】

自定义的插件要加上相应的权限配置，参考ik插件这里：

https://github.com/medcl/elasticsearch-analysis-ik/blob/master/src/main/resources/plugin-security.policy

# 5、低版本bug：关于Kibana6.8.2版本中文标题报告下载失败问题

在仪表板或者discover中保存的检索数据如果为中文标题的时候，点击共享，生成csv文件或者其他格式的报告时，总是报错失败，修改为英文标题则成功，但是业务要求要用中文。

这个问题在github上也搜到了问题，但是没有解决方式，想请教一个解决方式。

【回复】：

高版本已修复：
https://github.com/elastic/kibana/issues/41431

# 6、filebeat修改源码，支持多个output类型？？

软件版本；filebeat-7.5
运行环境；linux
场景/上下文；我现在希望通过修改源码，起到能够同时发送报文到两种类型的output，例如es/kafka
 
通过初步学习，了解到应该修改publisher这个部分，但是filebeat-7.5的源码基本和网上的学习资料有差异了，看的比较累

特来求助各位大神，filebeat是在哪一步识别应该用哪种发送方式呢？

【medcl 回复】

你还不如直接发给 kafka，从kafka 分出去，Filebeat 多个 Output 会变得很复杂，如果两个 Output，其中一个 Output挂了，怎么办，本地数据处理的 offset 要分别处理，采集端的压力和资源占用也要更高。

感谢回复 把代码读了下 确实不合适，决定统一采集 ，然后使用同一个client，在client内部做分支处理

# 7、安装ES的服务器，对于磁盘的块大小即block size有什么要求？

安装ES的服务器，对于磁盘的块大小即block size有什么要求？

服务器磁盘的文件系统在格式化的时候可以指定block size的大小，比如ext4 文件系统默认的块大小是4K，那么ES对于块大小有没有什么限制要求？多大的块对于ES来说更优？

【medcl 回复】

默认就好，这个一般不用单独配置

# 8、关于kibana 搜索时分词问题

当在kibana全文检索时，搜索的内容为  *@a-b 这种情况时，比如想匹配到的数据为： 
 
test1@a-b
test2@a-b
exec@a-b 
 
但是中间的  @  和  -  会被分词，检索出来的内容不对，求助应该如何写搜索语句

【medcl 回复】

两边加引号就可以了。如："test1@a-b"

切换为 Lucene 语法：字段.keyword: *@a-b

# 9、ES7.X压缩索引体积的方法还有哪些

老哥们，请教个问下elk场景下的磁盘空间优化问题。 
之前我们用的是es5.5 ，为了节约磁盘空间，开了best_compression，并且将很多type设置为了keyword。

但是新版的日志集群 用的是es7.4， 发现它废弃了_all ，也不支持指定索引类型了。 这种情况下，还有啥节约磁盘的好方法吗

【回复】
1、_source 关闭

2、doc_value : false

不知道你们的场景，可以查下这2个配置的含义，看看你们是否可以关闭

# 10、ES 错误更新字段【字段类型不匹配】

elasticsearch 6.1.1
 ```
mapping
 "server": {
       "type": "integer"
  }

 
写入数据：
	"server": 423093422,
    "name": "地铁站11",
    "time": 1583334489

执行更新 _update_by_query ，其中server 字段作为字符串更新，居然成功了
"script": {
    "inline": "ctx._source.name=params.name;ctx._source.server=params.server;",
    "params": {
      "name": "香雪新地铁131",
      "server": "42309301"
    },
    "lang": "painless"
  }


更新后：
"_source": {
      "server": "42309301",
      "name": "香雪新地铁131",
      "time": 1583334489
}
 ```
 为啥呢，按理说应该报个类型错误吧

【回复】

加上这个参数：     "coerce": false 来控制强制类型转换，默认是支持的。加上后：会报错：
"reason": "Integer value passed as String"
 
https://www.elastic.co/guide/en/elasticsearch/reference/current/coerce.html

# 11、为什么连续发起查询时，第二次居然会变慢？
我们项目刚开始用ES，在这一次的需求里，我们要根据一些filter条件获取某几个字段的平均值结果，是的，条件都是filter。
做测试时发现，不断执行同一个查询，第一遍稍慢，第二遍非常慢，后面几遍很快。
按我理解，第一次查时数据应该缓存下来了，应该是越来越快才对啊
 

这是为什么呢？
 
对了，我输入的filter在索引里没有匹配的，是空集

【wood大叔回复】

如果开了复制片，前两次查询会分别落到主片和复制片所在的节点。 是否有某个节点配置上或者性能有问题造成比较慢？

  查询的url里可以通过指定preference参数，让查询只针对某些shard或某些node，加这个参数排查下是否是上面说的问题。 

https://elasticsearch.cn/question/9802

# 12、【深入】为什么es写数据是先发请求到primary shard，再将请求转给replica shard

内容长，建议看链接内容。

https://elasticsearch.cn/question/9805

# 13、【这个脚本用的6】es自定义排序错乱问题
es6，请教大家，我想让id=1，2，3，4的四条数据排在最前面，剩下的数据再根据时间降序要怎么做呢？我用了function score设置了weight，然后根据评分降序再用创建时间降序，可以让符合id条件的排前面，但后面的数据时间顺序是错乱的。不知道问题出在哪里，或者说我这样做是行不通的？

【回复】

考虑用script sort吗？
 
先给答案，可以用这个script。
 ```
POST test/_search
{
  "sort": [
    {
      "_script": {
        "script": {
          "source": """String id = doc['_id'].getValue(); if ("1".equals(id) || "2".equals(id)) { return Integer.parseInt(id) * System.nanoTime(); } else { return doc['time'].getValue().toInstant().toEpochMilli(); }"""
        },
        "type": "number",
        "order": "desc"
      }
    }
  ]
}
 ```
准备数据
Mapping
 ```
PUT test
{
  "settings": {
    "index": {
      "number_of_shards": "1",
      "number_of_replicas": "0"
    }
  },
  "mappings": {
    "properties": {
      "id": {
        "type": "keyword"
      },
      "time": {
        "type": "date",
        "format": [
          "yyyy-MM-dd HH:mm:ss"
          ]
      }
    }
  }
}
 ```
数据：
 ```
POST test/_doc/1
{
  "name": "1",
  "time": "2020-02-02 09:00:00"
}

POST test/_doc/2
{
  "name": "2",
  "time": "2020-02-02 08:00:00"
}

POST test/_doc/3
{
  "name": "3",
  "time": "2020-02-02 05:00:00"
}

POST test/_doc/4
{
  "name": "4",
  "time": "2020-02-02 06:00:00"
}
 ```
简化一下题目，id=1，2的按顺序返回，也可以逆序，看你自己的需求，其他的按时间倒序。
思路：把id=1，2的单独拿出来给个大参数（比如当前时间），然后其他的按时间倒排。
分别对应
Integer.parseInt(id) * System.nanoTime();
和
doc['time'].getValue().toInstant().toEpochMilli();

# 14、批量写入多个index索引文档会生成一个段还是每个文档生成一个段或者相同index的文档会生成一个段？

批量写入多个index索引文档会生成一个段还是每个文档生成一个段或者相同index的文档会生成一个段？

Filesystem Cache新建的segment里面是包含了多个index的倒排索引数据还是一个索引对应一个段？比如。bulk请求里包含了对不同index的索引数据，他们会写入在一个段里吗？
 

【回复】
 ```
bulk请求到coordinator node上后每条数据会被分发到不同的primary shard上，在对应的节点index buffer上cache住。在达到refresh的阈值以后，相同的shard的数据会生成一个segment持久化。
 ```

# 15、添加单条数据时检查重复，重复就更新，不重复就添加

比如 有两个字段url和text的数据，在添加一条数据时看url字段有没有与已有数据重复的，重复就更新这条数据更新，不重复就添加。最好不能指定url为_id。
这是创建的索引：
 ```
PUT /index1
{
  "mappings": {
    "properties": {
      "url":    { "type": "keyword" },  
      "text":  { "type": "text"  }
    }
  }
}
假如添加一条数据：
POST /index1/_doc
{
  "url":"www.baidu.com",
  "text":"网页"
}
再添加数据：
POST /index1/_doc
{
  "url":"www.baidu.com",
  "text":"网页xxxxx"
}
因为url与已有的数据有相等的所以应该更新
 
再添加数据:
POST /index1/_doc
{
  "url":"www.hao123.com",
  "text":"网页xxxxx"
}
没有url与库中没有重复所以就添加

 ```

【回复】

1.这明显是需要一个url作为主键的啊，不能指定url作为id的原因是什么，用hash值也不行吗。
2.如果不行的话，在写入的时候进行一下查询，如果存在就更新，不存在就添加。

https://elasticsearch.cn/question/9818

铭毅推荐更多：

https://stackoverflow.com/questions/50871824/is-it-possible-to-have-a-computed-id-field-by-hashing-other-fields-of-the-docum/50872486

https://alexmarquardt.com/2018/07/23/deduplicating-documents-in-elasticsearch/


# 16、es7.3怎么生成日期相差天数的字段

初学者请教一下各位大佬，怎么使用es的script_fields生成一个文档的createTime字段与当前时间相差天数的字段？我打算使用这个字段调整召回的_score
下面附上mapping
```
{
  "mapping": {
    "properties": {
      "createTime": {
        "type": "date",
        "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
      },
      "id": {
        "type": "integer"
      },
      "title": {
        "type": "text",
        "analyzer": "ik_max_word",
        "search_analyzer": "ik_smart"
      }
    }
  }
}
随便拿一条数据
{
  "_index" : "article",
  "_type" : "_doc",
  "_id" : "12",
  "_seq_no" : 0,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "id" : 12,
    "createTime" : "2020-01-11 13:18:47",
    "title" : "如何学习Springboot"
  }
}

 ```
就是用这个createTime字段得出与目前相差多少天

【回复】
写了一个，不太优雅，你可以再优化一下，抛砖引玉。
```
{
	"query":{
		"match_all":{
			
		}
	},
	"script_fields":{
		"dis_date":{
			"script":{
				"lang":"painless",
				"source":"new Date().getTime() - doc['createTime'].value.toLocalDateTime().toInstant(ZoneOffset.of('+8')).toEpochMilli()"
			}
		}
	}
}

```

# 17、【数据建模】如何查询一个字符串里面数字在范围内
现在有一个索引 ，里面有一个 shenfen_no 字段为身份证号，类型为keyword, 现在想根据年月日范围查询 ，希望找到身份证号里面的年月日 满足范围查询的身份证号，
例如 身份证号 数据有 xxxxxx19890156xxxx， xxxxxx19930156xxxx ，xxxxxx20000156xxxx
现在范围查询为 19910101 到 19990101 ，这样查询出 xxxxxx19930156xxxx 就想这样查询 dsl 该怎么写

【回复】

https://elasticsearch.cn/question/9824

使用painless是可以实现的，但是不建议这么干，因为身份证号本身是为了作为唯一标识含义而存在的字符串字段，通过它来进行范围的操作是十分不合理的。

每个工具或者字段都有它本身的适用场景，只考虑能不能用的话很容易带来性能的下降或者资源浪费。就像拿水果刀去切菜切肉的感觉一样。。。

我认为比较正确的做法是建立并填充出生日期字段，通过此字段去查询。当然还要结合你所使用的具体场景去决定。

# 18、【脚本】expression脚本条件更新

我现在尝试通过expression脚本去对于索引进行条件更新但现在返还了didn't store source的错误。

请问有什么办法去修正呢？我现在用的是7.6的elasticsearch和kibana

【回复】
这里的语言应该是painless吧。你可以看一下在我的教程https://blog.csdn.net/UbuntuTo ... 81016里的例子。你直接搜一下_update_by_query
``` 
POST twitter/_update_by_query
{
  "script": {
    "source": "ctx._source.city = params.city;ctx._source.province = params.province;ctx._source.country = params.country",
    "lang": "painless",
    "params": {
      "city": "上海",
      "province": "上海",
      "country": "中国"
    },
    "query": {
      "match": {
        "user": "GB"
      }
    }
  }
}
```

# 19、【多表关联】我现在有两个索引库a,b。我想根据条件查a的索引库，前提是a索引库的id值必须在b索引库的id值中存在

【选型回复】
es是列存储的，2个索引库这样互联不建议。一般会做宽表，嵌套，父子表这样的处理。es只处理一个索引库，不支持2个库类似于left jionon的操作

# 20、【更新】es7.3 新增字段后，如何快速给新字段赋值

es7.3,项目数据大概有300多万，因为新需求给索引里新增了一个字段appid,想把老数据快速加上该字段并且赋值为1，之前使用：
curl -X POST "xxxxxxx:9200/index_name/_update_by_query?pretty" -H 'Content-Type: application/json' -d'
{
  "script": {
    "lang": "painless",
    "inline": "ctx._source.appid= 1"
  }
}

除了该方法还有其他方法么？使用这个方法，我执行了很长时间，而且最后报错了，没有改完。

【回复】
最快的方式 就是在kibana 使用index_pattern >>> script field 增加一個字段  value 設定為1就可以了。
當然如果可以建議還是在原本的source data 補上該字段。

感谢~kibana没有使用过，不太懂，最终我还是使用_update_by_query更新，之前更新没有加条件，更新一半会提示冲突，应该是因为有新文档中的数据appid已经有新值了，所以我加了exists条件，更新时没有提示冲突

# 21、es自动删除所有索引?o.e.c.m.MetaDataDeleteIndexService这是什么意思?请大神指点

【回复】

ES不会无缘故删除索引，应该是以下几个可能性：
- 有人执行了 "DELETE *"之类的操作
- 有外部的脚本定期执行API call清理数据
- 有curator或者ILM之类的

# 22、好文推荐

Elasticsearch 7.6 利用机器学习来检测文本语言

https://elasticsearch.cn/article/13678

使用Elasticsearch实现同段和同句搜索

https://elasticsearch.cn/article/13677

Beats：运用Elastic Stack分析COVID-19数据并进行可视化分析

https://elasticsearch.cn/question/9845

